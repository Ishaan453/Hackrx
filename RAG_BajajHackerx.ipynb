{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (run this cell first in Google Colab)\n",
        "!pip install pinecone-client sentence-transformers PyPDF2 python-docx openpyxl python-pptx pillow pytesseract pinecone faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "RIThx_3AFjmC",
        "outputId": "8afb5155-8b38-43a8-c584-6a1fb355f810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.7.14)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.14.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Collecting packaging>=21.3 (from pytesseract)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-docx, PyPDF2, pinecone-plugin-interface, packaging, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, python-pptx, pytesseract, pinecone-plugin-assistant, pinecone-client, nvidia-cusparse-cu12, nvidia-cudnn-cu12, faiss-cpu, pinecone, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed PyPDF2-3.0.1 XlsxWriter-3.2.5 faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 packaging-24.2 pinecone-7.3.0 pinecone-client-6.0.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7 pytesseract-0.3.13 python-docx-1.2.0 python-pptx-1.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "dbe105db65204c63a2f63313263d07e8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu astrapy pandas sentence-transformers requests langchain_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTlASSh3r4rv",
        "outputId": "3901fed9-5c4a-48ae-cdd0-f78a61b8f12f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: astrapy in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from astrapy) (2.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy) (0.28.1)\n",
            "Requirement already satisfied: pymongo>=3 in /usr/local/lib/python3.11/dist-packages (from astrapy) (4.13.2)\n",
            "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from astrapy) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from astrapy) (4.14.1)\n",
            "Requirement already satisfied: uuid6>=2024.1.12 in /usr/local/lib/python3.11/dist-packages (from astrapy) (2025.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.3.72)\n",
            "Requirement already satisfied: groq<1,>=0.29.0 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.30.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain_groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (1.33)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo>=3->astrapy) (2.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy) (4.1.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading langchain_groq-0.3.6-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: langchain_groq\n",
            "Successfully installed langchain_groq-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "import mimetypes\n",
        "from typing import List, Dict, Any, Optional\n",
        "import logging\n",
        "\n",
        "# Core libraries\n",
        "import pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# File processing libraries\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import openpyxl\n",
        "from pptx import Presentation\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import json\n",
        "import csv\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "QsuQtX5nFgba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHOY6SliEEkC"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "class FAISSRetriever:\n",
        "    def __init__(self, text: str, embedding_model: str = 'all-MiniLM-L6-v2', chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        \"\"\"\n",
        "        Initialize FAISS retriever with extracted text\n",
        "\n",
        "        Args:\n",
        "            text: The extracted text from PDF (your 'result' variable)\n",
        "            embedding_model: Sentence transformer model name\n",
        "            chunk_size: Size of text chunks\n",
        "            chunk_overlap: Overlap between chunks\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "\n",
        "        # Process text and create index\n",
        "        self.chunks = self._chunk_text(text)\n",
        "        self.embeddings = self._create_embeddings(self.chunks)\n",
        "        self.index = self._create_faiss_index(self.embeddings)\n",
        "\n",
        "        print(f\"✅ Created FAISS index with {len(self.chunks)} chunks\")\n",
        "\n",
        "    def _chunk_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        if len(text) <= self.chunk_size:\n",
        "            return [text]\n",
        "\n",
        "        chunks = []\n",
        "        start = 0\n",
        "\n",
        "        while start < len(text):\n",
        "            end = start + self.chunk_size\n",
        "            chunk = text[start:end]\n",
        "\n",
        "            # Try to end at sentence boundary\n",
        "            if end < len(text):\n",
        "                last_period = chunk.rfind('.')\n",
        "                last_newline = chunk.rfind('\\n')\n",
        "                boundary = max(last_period, last_newline)\n",
        "\n",
        "                if boundary > start + self.chunk_size // 2:\n",
        "                    chunk = text[start:start + boundary + 1]\n",
        "                    end = start + boundary + 1\n",
        "\n",
        "            chunks.append(chunk.strip())\n",
        "            start = end - self.chunk_overlap\n",
        "\n",
        "            if start >= len(text):\n",
        "                break\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _create_embeddings(self, chunks: List[str]) -> np.ndarray:\n",
        "        \"\"\"Create embeddings for all chunks\"\"\"\n",
        "        embeddings = self.embedding_model.encode(chunks, convert_to_numpy=True)\n",
        "        return embeddings\n",
        "\n",
        "    def _create_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
        "        \"\"\"Create FAISS index from embeddings\"\"\"\n",
        "        dimension = embeddings.shape[1]\n",
        "\n",
        "        # Use IndexFlatIP for cosine similarity (after normalization)\n",
        "        index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "\n",
        "        # Add embeddings to index\n",
        "        index.add(embeddings)\n",
        "\n",
        "        return index\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve most relevant chunks for a query\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "            top_k: Number of top chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            List of tuples (chunk_text, similarity_score)\n",
        "        \"\"\"\n",
        "        # Create query embedding\n",
        "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "        # Normalize query embedding\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search index\n",
        "        scores, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Return results\n",
        "        results = []\n",
        "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "            if idx < len(self.chunks):  # Valid index\n",
        "                results.append((self.chunks[idx], float(score)))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def retrieve_with_context(self, query: str, top_k: int = 3) -> List[dict]:\n",
        "        \"\"\"\n",
        "        Retrieve chunks with additional context information\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries with chunk info\n",
        "        \"\"\"\n",
        "        results = self.retrieve(query, top_k)\n",
        "\n",
        "        detailed_results = []\n",
        "        for i, (chunk, score) in enumerate(results):\n",
        "            detailed_results.append({\n",
        "                'rank': i + 1,\n",
        "                'chunk': chunk,\n",
        "                'similarity_score': score,\n",
        "                'chunk_length': len(chunk),\n",
        "                'preview': chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
        "            })\n",
        "\n",
        "        return detailed_results\n",
        "\n",
        "# Usage example:\n",
        "def create_retriever_from_result(result: str) -> FAISSRetriever:\n",
        "    \"\"\"\n",
        "    Create FAISS retriever from your extracted PDF text\n",
        "\n",
        "    Args:\n",
        "        result: Your extracted text variable\n",
        "\n",
        "    Returns:\n",
        "        FAISSRetriever instance\n",
        "    \"\"\"\n",
        "    retriever = FAISSRetriever(\n",
        "        text=result,\n",
        "        chunk_size=500,  # Adjust as needed\n",
        "        chunk_overlap=50  # Adjust as needed\n",
        "    )\n",
        "    return retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss"
      ],
      "metadata": {
        "id": "YADCdiJmEG8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_path = \"/content/BAJHLIP23020V012223.pdf\"\n",
        "import requests\n",
        "import io\n",
        "\n",
        "def extract_pdf_from_url(url: str) -> str:\n",
        "   \"\"\"Extract text from PDF URL\"\"\"\n",
        "   try:\n",
        "       # Download PDF from URL\n",
        "       response = requests.get(url)\n",
        "       response.raise_for_status()\n",
        "\n",
        "       # Create a file-like object from the response content\n",
        "       pdf_file = io.BytesIO(response.content)\n",
        "\n",
        "       # Extract text using PyPDF2\n",
        "       pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "       text = \"\"\n",
        "       for page in pdf_reader.pages:\n",
        "           text += page.extract_text() + \"\\n\"\n",
        "       return text\n",
        "   except Exception as e:\n",
        "       print(f\"Error extracting PDF from URL: {str(e)}\")\n",
        "       return \"\"\n",
        "\n",
        "# Usage:\n",
        "pdf_url = \"https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D\"\n",
        "\n",
        "# result = extract_pdf_from_url(pdf_url)\n",
        "# retriever = create_retriever_from_result(result)\n",
        "def extract_pdf_content(file_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF files\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting PDF content: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "# Process the file\n",
        "#result = extract_pdf_content(file_path)\n",
        "result = extract_pdf_from_url(pdf_url)\n",
        "print(\"PDF content extracted successfully.\")\n",
        "retriever = create_retriever_from_result(result)\n",
        "print(\"FAISS retriever created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P10qJ1tStJSy",
        "outputId": "d7aa5547-a2f5-4132-9725-85d81853aff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF content extracted successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created FAISS index with 252 chunks\n",
            "FAISS retriever created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Query for relevant chunks\n",
        "user_query = \"will I get the cover if my wife died while delivery of the child\"\n",
        "relevant_chunks = retriever.retrieve(user_query, top_k=3)\n",
        "\n",
        "# Step 3: Use the results\n",
        "for i, (chunk, score) in enumerate(relevant_chunks, 1):\n",
        "    print(f\"Chunk {i} (Score: {score:.3f}):\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t10X6K_qEHA9",
        "outputId": "ef8616df-dda2-4bc1-c465-c8a84f43a4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 (Score: 0.443):\n",
            ", for up to 3 months of age.  On attaining  3 months of age, the  New Born Baby  shall be covered only if \n",
            "specifically included in the Policy  mid-term and requisite premium paid to the Company.  \n",
            " \n",
            "Cover  \n",
            "Maternity Expenses  means;  \n",
            "a) Medical treatment expenses traceable to childbirth (including complicated deliveries and caesarean sections incurred during \n",
            "Hospitalization);  \n",
            "b) Expenses towards lawful medica l termination of pregnancy during the Policy Period.  \n",
            "Note:  Ectopic pregnancy i\n",
            "--------------------------------------------------\n",
            "Chunk 2 (Score: 0.415):\n",
            "within a  Waiting Period of twenty -four (24) months.  However, the Waiting Period may be waived \n",
            "only in the case of delivery, miscarriage or abortion induced by accident.   \n",
            "3. Delivery or lawful medical termination of pregnancy limited to  two deliveries or terminations or either  has been paid under \n",
            "the Policy an d its Renewals.  \n",
            "4. More than one delivery or lawful medical termination of pregnancy during a single Policy Period.  \n",
            "5. Maternity Expenses of Surrogate Mother, unless claim is\n",
            "--------------------------------------------------\n",
            "Chunk 3 (Score: 0.401):\n",
            "ng the Policy Period.  \n",
            "Note:  Ectopic pregnancy is covered under ‘In -patient treatment’, provided such pregnancy is established by medical reports.  \n",
            "  \n",
            "Exclusions  \n",
            "The Company shall not be liable to make any payment under the cover in respect of  Maternity Expenses  incurred in connection with \n",
            "or in respect of:  \n",
            "1. Covered female Insured Person  below eighteen (18) years  and above forty -five (45) years of age.  \n",
            "2. Delivery or termination within a  Waiting Period of twenty -four (24) mon\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#llama-3.1-8b-instant\n",
        "# For reasoning models\n",
        "# llama-4-maverick-17b-128e-instruct\n",
        "\n",
        "# reasoning model - setting the max reasoning tokens and all for efficiency\n",
        "\n",
        "import json\n",
        "from groq import Groq\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "zdrj2YX3EHDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "import json\n",
        "\n",
        "def process_pdf_queries(pdf_url: str, queries_json: str, groq_api_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Process PDF from URL and answer queries one by one using vector search + ChatGroq\n",
        "    Returns concise, point-to-point answers\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize ChatGroq with optimized settings for faster response\n",
        "        llm = ChatGroq(\n",
        "            groq_api_key=groq_api_key,\n",
        "            model_name=\"llama-3.1-8b-instant\",\n",
        "            temperature=0.1,\n",
        "            max_tokens=150  # Reduced for shorter answers\n",
        "        )\n",
        "\n",
        "        # Extract PDF content (done once)\n",
        "        result = extract_pdf_from_url(pdf_url)\n",
        "        if not result:\n",
        "            return json.dumps({\"error\": \"Failed to extract PDF content\"})\n",
        "\n",
        "        # Create retriever (done once)\n",
        "        retriever = create_retriever_from_result(result)\n",
        "\n",
        "        # Parse queries\n",
        "        queries_data = json.loads(queries_json)\n",
        "        queries = queries_data.get(\"queries\", [])\n",
        "\n",
        "        answers = {}\n",
        "\n",
        "        # Optimized examples for concise responses\n",
        "        examples = \"\"\"Examples of concise policy answers:\n",
        "\n",
        "Q: What is the grace period for premium payment?\n",
        "A: 30 days grace period after due date.\n",
        "\n",
        "Q: What is the waiting period for pre-existing diseases?\n",
        "A: 36 months continuous coverage required.\n",
        "\n",
        "Q: Does this policy cover maternity expenses?\n",
        "A: Yes, after 24 months continuous coverage. Limited to 2 deliveries per policy period.\n",
        "\n",
        "Q: What is the waiting period for cataract surgery?\n",
        "A: 2 years waiting period.\"\"\"\n",
        "\n",
        "        # Process each query individually\n",
        "        for query in queries:\n",
        "            # Get relevant chunks for this specific query\n",
        "            relevant_chunks = retriever.retrieve(query, top_k=2)  # Reduced to 2 for faster processing\n",
        "\n",
        "            # Combine chunks for context\n",
        "            context = \"\\n\\n\".join([chunk for chunk, score in relevant_chunks])\n",
        "\n",
        "            # Create focused prompt for concise answers\n",
        "            prompt = f\"\"\"{examples}\n",
        "\n",
        "Based on the context below, provide a concise, specific answer:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Q: {query}\n",
        "A:\"\"\"\n",
        "\n",
        "            # Get answer from ChatGroq\n",
        "            response = llm.invoke(prompt)\n",
        "            answer = response.content.strip()\n",
        "\n",
        "            # Clean the answer - remove any Q: and A: prefixes if present\n",
        "            if answer.startswith(\"Q:\"):\n",
        "                # Find the A: part and extract only the answer\n",
        "                a_index = answer.find(\"A:\")\n",
        "                if a_index != -1:\n",
        "                    answer = answer[a_index + 2:].strip()\n",
        "            elif answer.startswith(\"A:\"):\n",
        "                # Remove A: prefix\n",
        "                answer = answer[2:].strip()\n",
        "\n",
        "            # Store only the clean answer\n",
        "            answers[query] = answer\n",
        "\n",
        "        # Return compiled answers in clean JSON structure\n",
        "        return json.dumps({\"answers\": answers}, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "pdf_url = \"https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D\"\n",
        "\n",
        "queries_json = json.dumps({\n",
        "    \"queries\": [\n",
        "        \"will I get the cover if my wife died while delivery of the child\",\n",
        "        \"what is the premium amount\",\n",
        "        \"what are the exclusions in this policy\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Execute\n",
        "result = process_pdf_queries(pdf_url, queries_json, groq_api_key)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "WZKZPKrfEHFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a33748-0bde-454b-a072-949e284f80a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created FAISS index with 252 chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answers\": {\n",
            "    \"will I get the cover if my wife died while delivery of the child\": \"No, coverage for maternity expenses is only available if the policyholder is alive.\",\n",
            "    \"what is the premium amount\": \"However, the context provided does not mention a specific premium amount. It only mentions various discounts and co-payment options. To provide a concise answer, I would need more information about the premium amount before the discounts or co-payments are applied.\\n\\nIf you provide the original premium amount, I can give you a concise answer in the format you requested. For example:\\n\\nQ: What is the premium amount after a 10% discount?\\nA: 90% of original premium amount.\\n\\nPlease provide the original premium amount for a more accurate answer.\",\n",
            "    \"what are the exclusions in this policy\": \"Critical illnesses and/or their symptoms present before policy inception or manifesting within 90 days from inception, and any claims made during a break in coverage.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the secret\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "  print(\"success\")\n"
      ],
      "metadata": {
        "id": "QN1W8yTBEHI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ea5b86-d065-4648-ce69-a21fcf99e745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "\n",
        "# Execute the function\n",
        "result = process_pdf_queries(pdf_url, queries_json, groq_api_key)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "4wf_YV41EHLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qja_2Tb0EHOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGMh8f_Ovfz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUkEWKX7EHRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}